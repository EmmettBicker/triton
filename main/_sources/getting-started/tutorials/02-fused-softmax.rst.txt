
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-60

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 61-69

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 71-80

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 80-110

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 111-112

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 112-186

.. code-block:: Python


    device = torch.cuda.current_device()
    properties = driver.active.utils.get_device_properties(device)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software piepling stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel, num_programs = kernels.get(BLOCK_SIZE, (None, 0))
        if kernel is None:
            kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                           num_stages=num_stages, num_warps=num_warps, grid=(1, ))
            kernel._init_handles()
            n_regs = kernel.n_regs
            size_smem = kernel.metadata.shared
            if is_hip():
                # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
                # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
                # ISA SECTION (3.6.4 for CDNA3)
                # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
                # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
                # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
                # not required to be equal numbers of both types.
                if is_cdna():
                    NUM_GPRS = NUM_REGS * 2

                # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
                # When we divide this number with WARP_SIZE we get maximum number of waves that can
                # execute on a CU (multi-processor)  in parallel.
                MAX_NUM_THREADS = properties["max_threads_per_sm"]
                max_num_waves = MAX_NUM_THREADS // WARP_SIZE
                occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
            else:
                occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
            occupancy = min(occupancy, SIZE_SMEM // size_smem)
            num_programs = NUM_SM * occupancy
            kernels[BLOCK_SIZE] = (kernel, num_programs)

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](
            y,
            x,
            x.stride(0),
            y.stride(0),
            n_rows,
            n_cols,
        )
        return y









.. GENERATED FROM PYTHON SOURCE LINES 187-189

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 191-193

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 193-200

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device='cuda')
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 201-202

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 204-209

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 209-240

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch'],  # possible values for `line_arg``
            line_names=[
                "Triton",
                "Torch",
            ],  # label name for the lines
            styles=[('blue', '-'), ('green', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device='cuda', dtype=torch.float32)
        stream = torch.cuda.Stream()
        torch.cuda.set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch
    0     256.0   482.356812   708.825708
    1     384.0   617.286889   823.484793
    2     512.0   753.681869   918.529290
    3     640.0   795.897488   960.737833
    4     768.0   881.272302  1028.499308
    5     896.0   937.047712  1059.997443
    6    1024.0   988.844132  1111.616400
    7    1152.0  1099.868703   614.296023
    8    1280.0  1144.322120   669.080269
    9    1408.0  1159.328078   726.156541
    10   1536.0  1193.966547   778.828221
    11   1664.0  1211.714758   812.342194
    12   1792.0  1242.688872   855.153626
    13   1920.0  1247.314774   908.084564
    14   2048.0  1272.246232   960.414898
    15   2176.0  1263.047054   973.361753
    16   2304.0  1274.944696  1011.405944
    17   2432.0  1293.473716  1053.331571
    18   2560.0  1300.362375  1083.000315
    19   2688.0  1316.398138  1106.805095
    20   2816.0  1325.944418  1130.582265
    21   2944.0  1321.519901  1167.058509
    22   3072.0  1352.723989  1181.705661
    23   3200.0  1355.303317  1195.287087
    24   3328.0  1358.238894  1222.504467
    25   3456.0  1378.433023  1249.987972
    26   3584.0  1372.374912  1257.132608
    27   3712.0  1381.209302  1269.907308
    28   3840.0  1385.197752  1296.966005
    29   3968.0  1388.206550  1316.792812
    30   4096.0  1399.191798  1326.885100
    31   4224.0  1337.385258  1163.547145
    32   4352.0  1334.609054  1176.923631
    33   4480.0  1349.813624  1185.038901
    34   4608.0  1359.956232  1194.932202
    35   4736.0  1358.132044  1200.283762
    36   4864.0  1378.599787  1219.795530
    37   4992.0  1369.266870  1237.515002
    38   5120.0  1375.989845  1252.517391
    39   5248.0  1372.896431  1257.259618
    40   5376.0  1383.187706  1291.117866
    41   5504.0  1383.449571  1301.940418
    42   5632.0  1388.567525  1313.603033
    43   5760.0  1396.847619  1326.275217
    44   5888.0  1387.759804  1343.079340
    45   6016.0  1400.629067  1352.018638
    46   6144.0  1407.240023  1371.417043
    47   6272.0  1415.804846  1374.251907
    48   6400.0  1419.119660  1385.390946
    49   6528.0  1414.057414  1393.930695
    50   6656.0  1422.477491  1400.784060
    51   6784.0  1414.356601  1412.080041
    52   6912.0  1428.588631  1422.919888
    53   7040.0  1423.867279  1432.318979
    54   7168.0  1426.124473  1435.765087
    55   7296.0  1428.591472  1444.282250
    56   7424.0  1432.377276  1447.993087
    57   7552.0  1429.102203  1454.419309
    58   7680.0  1436.067757  1458.727942
    59   7808.0  1432.593580  1464.161424
    60   7936.0  1436.542725  1469.070390
    61   8064.0  1442.619920  1474.761292
    62   8192.0  1437.472182  1484.913836
    63   8320.0  1385.164831  1403.036264
    64   8448.0  1381.428106  1406.258506
    65   8576.0  1396.665172  1394.381434
    66   8704.0  1393.988189  1401.526499
    67   8832.0  1380.275475  1402.998443
    68   8960.0  1396.413932  1412.388662
    69   9088.0  1410.778436  1414.172872
    70   9216.0  1404.417512  1421.275351
    71   9344.0  1398.218658  1424.048640
    72   9472.0  1401.655183  1433.859529
    73   9600.0  1397.745720  1435.188746
    74   9728.0  1401.607729  1442.970108
    75   9856.0  1415.461304  1444.384132
    76   9984.0  1395.001400  1453.389955
    77  10112.0  1410.920939  1453.562095
    78  10240.0  1420.850227  1466.896788
    79  10368.0  1408.994009  1464.542204
    80  10496.0  1409.899980  1464.465555
    81  10624.0  1410.703208  1468.052592
    82  10752.0  1403.216518  1472.072377
    83  10880.0  1400.538116  1480.706801
    84  11008.0  1420.862503  1476.701287
    85  11136.0  1424.216158  1482.978311
    86  11264.0  1429.044228  1487.422378
    87  11392.0  1417.667949  1488.612481
    88  11520.0  1423.776632  1493.397746
    89  11648.0  1423.060608  1497.498630
    90  11776.0  1434.102978  1501.273155
    91  11904.0  1444.432658  1508.257179
    92  12032.0  1427.081051  1507.139674
    93  12160.0  1419.761330  1513.501617
    94  12288.0  1432.770897  1393.470372
    95  12416.0  1446.073908  1389.636673
    96  12544.0  1443.113139  1392.318943
    97  12672.0  1447.579278  1393.506259




.. GENERATED FROM PYTHON SOURCE LINES 241-245

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 23.392 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

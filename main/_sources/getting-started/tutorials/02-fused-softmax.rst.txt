
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "getting-started/tutorials/02-fused-softmax.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_getting-started_tutorials_02-fused-softmax.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_getting-started_tutorials_02-fused-softmax.py:


Fused Softmax
=============

In this tutorial, you will write a fused softmax operation that is significantly faster
than PyTorch's native op for a particular class of matrices: those whose rows can fit in
the GPU's SRAM.

In doing so, you will learn about:

* The benefits of kernel fusion for bandwidth-bound operations.

* Reduction operators in Triton.

.. GENERATED FROM PYTHON SOURCE LINES 18-23

Motivations
-----------

Custom GPU kernels for elementwise additions are educationally valuable but won't get you very far in practice.
Let us consider instead the case of a simple (numerically stabilized) softmax operation:

.. GENERATED FROM PYTHON SOURCE LINES 23-60

.. code-block:: Python


    import torch

    import triton
    import triton.language as tl
    from triton.runtime import driver


    def is_hip():
        return triton.runtime.driver.active.get_current_target().backend == "hip"


    def is_cdna():
        return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',
                                                                                       'gfx90a', 'gfx908')


    def naive_softmax(x):
        """Compute row-wise softmax of X using native pytorch

        We subtract the maximum element in order to avoid overflows. Softmax is invariant to
        this shift.
        """
        # read  MN elements ; write M  elements
        x_max = x.max(dim=1)[0]
        # read MN + M elements ; write MN elements
        z = x - x_max[:, None]
        # read  MN elements ; write MN elements
        numerator = torch.exp(z)
        # read  MN elements ; write M  elements
        denominator = numerator.sum(dim=1)
        # read MN + M elements ; write MN elements
        ret = numerator / denominator[:, None]
        # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements
        return ret









.. GENERATED FROM PYTHON SOURCE LINES 61-69

When implemented naively in PyTorch, computing :code:`y = naive_softmax(x)` for :math:`x \in R^{M \times N}`
requires reading :math:`5MN + 2M` elements from DRAM and writing back :math:`3MN + 2M` elements.
This is obviously wasteful; we'd prefer to have a custom "fused" kernel that only reads
X once and does all the necessary computations on-chip.
Doing so would require reading and writing back only :math:`MN` bytes, so we could
expect a theoretical speed-up of ~4x (i.e., :math:`(8MN + 4M) / 2MN`).
The `torch.jit.script` flags aims to perform this kind of "kernel fusion" automatically
but, as we will see later, it is still far from ideal.

.. GENERATED FROM PYTHON SOURCE LINES 71-80

Compute Kernel
--------------

Our softmax kernel works as follows: each program loads a set of rows of the input matrix X strided by number of programs,
normalizes it and writes back the result to the output Y.

Note that one important limitation of Triton is that each block must have a
power-of-two number of elements, so we need to internally "pad" each row and guard the
memory operations properly if we want to handle any possible input shapes:

.. GENERATED FROM PYTHON SOURCE LINES 80-110

.. code-block:: Python



    @triton.jit
    def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,
                       num_stages: tl.constexpr):
        # starting row of the program
        row_start = tl.program_id(0)
        row_step = tl.num_programs(0)
        for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):
            # The stride represents how much we need to increase the pointer to advance 1 row
            row_start_ptr = input_ptr + row_idx * input_row_stride
            # The block size is the next power of two greater than n_cols, so we can fit each
            # row in a single block
            col_offsets = tl.arange(0, BLOCK_SIZE)
            input_ptrs = row_start_ptr + col_offsets
            # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols
            mask = col_offsets < n_cols
            row = tl.load(input_ptrs, mask=mask, other=-float('inf'))
            # Subtract maximum for numerical stability
            row_minus_max = row - tl.max(row, axis=0)
            # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)
            numerator = tl.exp(row_minus_max)
            denominator = tl.sum(numerator, axis=0)
            softmax_output = numerator / denominator
            # Write back output to DRAM
            output_row_start_ptr = output_ptr + row_idx * output_row_stride
            output_ptrs = output_row_start_ptr + col_offsets
            tl.store(output_ptrs, softmax_output, mask=mask)









.. GENERATED FROM PYTHON SOURCE LINES 111-112

We can create a helper function that enqueues the kernel and its (meta-)arguments for any given input tensor.

.. GENERATED FROM PYTHON SOURCE LINES 112-186

.. code-block:: Python


    device = torch.cuda.current_device()
    properties = driver.active.utils.get_device_properties(device)
    NUM_SM = properties["multiprocessor_count"]
    NUM_REGS = properties["max_num_regs"]
    SIZE_SMEM = properties["max_shared_mem"]
    WARP_SIZE = properties["warpSize"]
    target = triton.runtime.driver.active.get_current_target()
    kernels = {}


    def softmax(x):
        n_rows, n_cols = x.shape

        # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`
        BLOCK_SIZE = triton.next_power_of_2(n_cols)

        # Another trick we can use is to ask the compiler to use more threads per row by
        # increasing the number of warps (`num_warps`) over which each row is distributed.
        # You will see in the next tutorial how to auto-tune this value in a more natural
        # way so you don't have to come up with manual heuristics yourself.
        num_warps = 8

        # Number of software piepling stages.
        num_stages = 4 if SIZE_SMEM > 200000 else 2

        # Allocate output
        y = torch.empty_like(x)

        # pre-compile kernel to get register usage and compute thread occupancy.
        kernel, num_programs = kernels.get(BLOCK_SIZE, (None, 0))
        if kernel is None:
            kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,
                                           num_stages=num_stages, num_warps=num_warps, grid=(1, ))
            kernel._init_handles()
            n_regs = kernel.n_regs
            size_smem = kernel.metadata.shared
            if is_hip():
                # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.
                # However, this is not always the case. In most cases all registers can be used as regular purpose registers.
                # ISA SECTION (3.6.4 for CDNA3)
                # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used
                # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total
                # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is
                # not required to be equal numbers of both types.
                if is_cdna():
                    NUM_GPRS = NUM_REGS * 2

                # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.
                # When we divide this number with WARP_SIZE we get maximum number of waves that can
                # execute on a CU (multi-processor)  in parallel.
                MAX_NUM_THREADS = properties["max_threads_per_sm"]
                max_num_waves = MAX_NUM_THREADS // WARP_SIZE
                occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps
            else:
                occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)
            occupancy = min(occupancy, SIZE_SMEM // size_smem)
            num_programs = NUM_SM * occupancy
            kernels[BLOCK_SIZE] = (kernel, num_programs)

        num_programs = min(num_programs, n_rows)

        # Create a number of persistent programs.
        kernel[(num_programs, 1, 1)](
            y,
            x,
            x.stride(0),
            y.stride(0),
            n_rows,
            n_cols,
        )
        return y









.. GENERATED FROM PYTHON SOURCE LINES 187-189

Unit Test
---------

.. GENERATED FROM PYTHON SOURCE LINES 191-193

We make sure that we test our kernel on a matrix with an irregular number of rows and columns.
This will allow us to verify that our padding mechanism works.

.. GENERATED FROM PYTHON SOURCE LINES 193-200

.. code-block:: Python


    torch.manual_seed(0)
    x = torch.randn(1823, 781, device='cuda')
    y_triton = softmax(x)
    y_torch = torch.softmax(x, axis=1)
    assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)








.. GENERATED FROM PYTHON SOURCE LINES 201-202

As expected, the results are identical.

.. GENERATED FROM PYTHON SOURCE LINES 204-209

Benchmark
---------

Here we will benchmark our operation as a function of the number of columns in the input matrix -- assuming 4096 rows.
We will then compare its performance against (1) :code:`torch.softmax` and (2) the :code:`naive_softmax` defined above.

.. GENERATED FROM PYTHON SOURCE LINES 209-240

.. code-block:: Python



    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['N'],  # argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 100)],  # different possible values for `x_name`
            line_arg='provider',  # argument name whose value corresponds to a different line in the plot
            line_vals=['triton', 'torch'],  # possible values for `line_arg``
            line_names=[
                "Triton",
                "Torch",
            ],  # label name for the lines
            styles=[('blue', '-'), ('green', '-')],  # line styles
            ylabel="GB/s",  # label name for the y-axis
            plot_name="softmax-performance",  # name for the plot. Used also as a file name for saving the plot.
            args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`
        ))
    def benchmark(M, N, provider):
        x = torch.randn(M, N, device='cuda', dtype=torch.float32)
        stream = torch.cuda.Stream()
        torch.cuda.set_stream(stream)
        if provider == 'torch':
            ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))
        if provider == 'triton':
            ms = triton.testing.do_bench(lambda: softmax(x))
        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
        return gbps(ms)


    benchmark.run(show_plots=True, print_data=True)




.. image-sg:: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :alt: 02 fused softmax
   :srcset: /getting-started/tutorials/images/sphx_glr_02-fused-softmax_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    softmax-performance:
              N       Triton        Torch
    0     256.0   475.582028   709.507845
    1     384.0   620.297547   807.550074
    2     512.0   755.011407   923.149977
    3     640.0   798.314730   960.967023
    4     768.0   877.879092  1035.435543
    5     896.0   941.269492  1068.102732
    6    1024.0   998.849054  1112.022904
    7    1152.0  1101.215190   615.860477
    8    1280.0  1146.237017   671.702000
    9    1408.0  1152.554940   725.864755
    10   1536.0  1190.862551   778.597347
    11   1664.0  1218.774560   816.886189
    12   1792.0  1241.676874   855.072909
    13   1920.0  1248.629627   908.361205
    14   2048.0  1278.776158   960.866211
    15   2176.0  1265.699968   976.712477
    16   2304.0  1267.319213  1012.660497
    17   2432.0  1297.209456  1055.367060
    18   2560.0  1301.783483  1088.877948
    19   2688.0  1309.770621  1101.057225
    20   2816.0  1326.795147  1129.699291
    21   2944.0  1324.824222  1166.340376
    22   3072.0  1356.368777  1188.784633
    23   3200.0  1356.854313  1195.846499
    24   3328.0  1359.238371  1225.569333
    25   3456.0  1370.975113  1246.297304
    26   3584.0  1378.111064  1261.060356
    27   3712.0  1383.283452  1269.205063
    28   3840.0  1383.629949  1302.510923
    29   3968.0  1393.293851  1312.907978
    30   4096.0  1394.045845  1326.116670
    31   4224.0  1331.795871  1161.025720
    32   4352.0  1340.972114  1177.806824
    33   4480.0  1350.627775  1184.268153
    34   4608.0  1367.743933  1196.011567
    35   4736.0  1360.062802  1201.975746
    36   4864.0  1376.755391  1221.827095
    37   4992.0  1374.344367  1233.049922
    38   5120.0  1376.387834  1249.484411
    39   5248.0  1371.788678  1256.407420
    40   5376.0  1374.051357  1283.993642
    41   5504.0  1380.875245  1296.408636
    42   5632.0  1385.022044  1313.924400
    43   5760.0  1396.940976  1321.105381
    44   5888.0  1388.435715  1339.897084
    45   6016.0  1401.274717  1353.198156
    46   6144.0  1406.251786  1371.644404
    47   6272.0  1412.777170  1374.857676
    48   6400.0  1419.536478  1390.927445
    49   6528.0  1418.515515  1392.408134
    50   6656.0  1426.062750  1407.001758
    51   6784.0  1409.633489  1410.753505
    52   6912.0  1426.607668  1424.614967
    53   7040.0  1418.487573  1430.703016
    54   7168.0  1428.675543  1437.036733
    55   7296.0  1432.061999  1442.909180
    56   7424.0  1425.799408  1443.647906
    57   7552.0  1425.522020  1454.185993
    58   7680.0  1433.821969  1457.825863
    59   7808.0  1434.344798  1462.721757
    60   7936.0  1436.217135  1466.654578
    61   8064.0  1435.797754  1470.185770
    62   8192.0  1438.826967  1481.910465
    63   8320.0  1387.438986  1401.613175
    64   8448.0  1379.188963  1405.759113
    65   8576.0  1397.794566  1395.097608
    66   8704.0  1389.929127  1399.256112
    67   8832.0  1383.244878  1401.746769
    68   8960.0  1399.920805  1413.496839
    69   9088.0  1410.187313  1416.918327
    70   9216.0  1404.721521  1426.184675
    71   9344.0  1401.666549  1424.569930
    72   9472.0  1396.986964  1434.118974
    73   9600.0  1393.330484  1434.548971
    74   9728.0  1401.651443  1440.377049
    75   9856.0  1412.972180  1444.409352
    76   9984.0  1398.220839  1450.396814
    77  10112.0  1415.526795  1453.556299
    78  10240.0  1419.113256  1466.902617
    79  10368.0  1409.545758  1465.539335
    80  10496.0  1415.530410  1464.700585
    81  10624.0  1413.977049  1470.494903
    82  10752.0  1407.250241  1471.046505
    83  10880.0  1396.587315  1482.015000
    84  11008.0  1419.277007  1475.731901
    85  11136.0  1421.688839  1488.944482
    86  11264.0  1433.705703  1485.007623
    87  11392.0  1420.821603  1489.035250
    88  11520.0  1422.300824  1491.351389
    89  11648.0  1428.154532  1496.537433
    90  11776.0  1428.452476  1501.298048
    91  11904.0  1446.267505  1504.585852
    92  12032.0  1423.586658  1508.763984
    93  12160.0  1418.171197  1511.928563
    94  12288.0  1435.419390  1390.238314
    95  12416.0  1451.368095  1390.831020
    96  12544.0  1445.352706  1395.545782
    97  12672.0  1450.199419  1394.605509




.. GENERATED FROM PYTHON SOURCE LINES 241-245

In the above plot, we can see that:
 - Triton is 4x faster than the Torch JIT. This confirms our suspicions that the Torch JIT does not do any fusion here.
 - Triton is noticeably faster than :code:`torch.softmax` -- in addition to being **easier to read, understand and maintain**.
   Note however that the PyTorch `softmax` operation is more general and will work on tensors of any shape.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 23.336 seconds)


.. _sphx_glr_download_getting-started_tutorials_02-fused-softmax.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02-fused-softmax.ipynb <02-fused-softmax.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02-fused-softmax.py <02-fused-softmax.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: 02-fused-softmax.zip <02-fused-softmax.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
